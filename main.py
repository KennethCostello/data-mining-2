import time
import creds
import praw
import pprint as pp
import pandas as pd
import string
import numpy as np
from methods import lookAtSubreddit, harvestCommentReplies, cleanText
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

start = time.time()

def main():
    #set subreeddits
    df1 = lookAtSubreddit("Dublin")
    df2 = lookAtSubreddit("Ireland")

    #get ids from dataframe
    id_list1 = df1.ids.to_numpy()
    id_list2 = df2.ids.to_numpy()

    #pass ids to get comments
    dfText1 = harvestCommentReplies(id_list1)
    dfText2 = harvestCommentReplies(id_list2)

    #join comments + titles to one dataframe
    df = pd.concat([df1, dfText1, df2, dfText2])

    #clean text
    df ['vsm'] = df.text.apply(cleanText)

    #get length of text
    df ['vsmlen'] = df.vsm.apply(len)

    #show countplot for characters in Comments
    plt.figure(figsize = (15,8))
    sns.countplot(data=df, x="vsmlen")
    plt.title('Comment Lengths', fontsize = 18)
    plt.xlabel('Characters per Comment', fontsize = 12)
    plt.ylabel('Number of Comments', fontsize = 12)
    plt.show()

    #declaring number of topics to be created from text data
    total_topics = 15

    #Fitting count vectorizer
    vectoriser = CountVectorizer()
    X = vectoriser.fit_transform(df.vsm)

    #getting the terms generated by count vectorizer
    terms = vectoriser.get_feature_names_out()

    # implement Latent Drichilet Allocation
    lda = LatentDirichletAllocation(n_components=total_topics, max_iter = 15,
                                        learning_method = "online", verbose = True,
                                         random_state = 1234)
    lda.fit_transform(X)
    #declaring number of terms we need per topic
    terms_count = 25
    #Looping over lda components to get topics and their related terms with high probabilities
    for idx,topic in enumerate(lda.components_):
        print('Topic# ',idx+1)
        abs_topic = abs(topic)
        topic_terms = [[terms[i],topic[i]] for i in abs_topic.argsort()[:-terms_count-1:-1]]
        topic_terms_sorted = [[terms[i], topic[i]] for i in abs_topic.argsort()[:-terms_count - 1:-1]]
        topic_words = []
        for i in range(terms_count):
            topic_words.append(topic_terms_sorted[i][0])
        print(','.join( word for word in topic_words))
        print("")
        dict_word_frequency = {}
        for i in range(terms_count):
            dict_word_frequency[topic_terms_sorted[i][0]] = topic_terms_sorted[i][1]
        wcloud = WordCloud(background_color="white",mask=None, max_words=100,\
                            max_font_size=60,min_font_size=10,prefer_horizontal=0.9,
                            contour_width=3,contour_color='black')
        wcloud.generate_from_frequencies(dict_word_frequency)
        #export topics as png
        plt.imshow(wcloud, interpolation='bilinear')
        plt.axis("off")
        plt.savefig("Topic{}.png".format(idx+1), format="png")
if __name__ == "__main__":
    main()

print("\n"+ 40*"*")
print(time.time()-start)
print( 40*"*"+ "\n")
